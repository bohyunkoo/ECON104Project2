---
title: "ECON104 Assignment 2"
author: "Ahjin Kim, Jimin Kim, Bohyun Koo, Miguel Luis Martinez"
date: "May 12, 2025"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
CPI_data <- read.csv("C:/Users/15654/Documents/US CPI.csv")
cpi <- CPI_data$CPI
cpi_ts <- ts(cpi, start = c(1913, 1), end = c(2021, 01), frequency = 12)
```

## PART 1 - Time Series and Autocorrelation

1. Exploratory Data Analysis

\textcolor{blue}{(a) Briefly discuss the question you are trying to answer.}

The question we are trying to answer is, "How was inflation, as measured by the Consumer Price Index (CPI), evolved over time, and what model best captures its underlying dynamics for forecasting purposes?"

\textcolor{blue}{(b) Cite the dataset and give a summary of what the dataset is about; make sure it is a time-series.}

We got our dataset from Kaggle. The data used in this analysis is sourced from the U.S. Bureau of Labor Statistics, and the specific dataset is owned and managed by Arpit Verma. 
Verma, Arpit. (2021). *U.S. Inflation Data* [Data set]. https://www.kaggle.com/datasets/varpit94/us-inflation-data-updated-till-may-2021/data 

\textcolor{blue}{(c) First check for completeness and consistency of the data (if there are NAs or missing observations, replace with the value of the previous observation; make a note of this)}

```{r}
which(is.na(CPI_data))
```
The output of integer(0) signifies that the data is complete and there are no missing observations.

\textcolor{blue}{(d) Provide descriptive analyses of your variables. This should include the histogram with overlying density, boxplots, cross correlation. All figures/statistics must include comments.}

```{r histogram-density, echo=TRUE}
library(ggplot2)

ggplot(CPI_data, aes(x = CPI)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "skyblue", color = "black") +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Histogram of CPI with Density Curve", x = "CPI", y = "Density")

```
The distribution of CPI values is right-skewed. This reflects the cumulative nature of inflation, with CPI values increasing over the years. The density curve peaks on the left, then tapers off toward the right, reinforcing the right-skewed nature of the data.


```{r}
ggplot(CPI_data, aes(x=CPI)) +
  geom_boxplot(fill="lightblue", color="darkgreen") +
  labs(title="Boxplot of CPI", x="CPI")

summary(CPI_data)
```
The boxplot shows that the median lies closer to the bottom of the interquartile range. The whiskers extend toward higher CPI values, but there are no outliers. 

The median is 33.10 while the mean is 82.64, which is much greater than the median, reinforcing that the data is right skewed. The range is 263.3, and the IQR is 130.3. We did not conduct a cross-correlation because our data analysis is univariate. 



2. Exploratory Data Analysis

\textcolor{blue}{(a) With tsdisplay or ggtsdisplay, for each variable, use its time series plot, ACF and PACF to comment on its stationarity (you can also decompose the time series; note if there is seasonality). To supplement this, use the appropriate Dickey-Fuller (unit root) test, to determine whether or not it is stationary. Note using its PACF what the suspected order might be.}

```{r}
library(tidyverse)
library(lubridate)
library(forecast)
library(tseries)
library(ggfortify)
library(ggpubr)

tsdisplay(cpi_ts, main="Original CPI Time Series")
```

There is a clear upward trend in CPI over time, which shows that the data is non-stationary because this would violate the stationarity assumption that mean is constant over time. The autocorrelation function (ACF) remains very high (close to 1) for all lags and decays slowly, which is a sign of non-stationarity. 

```{r}
adf.test(cpi_ts)
```
The Augmented Dickey-Fuller Test supports our conclusion of non-stationarity. The null hypothesis of this test is that the data is non-stationary. The p-value of 0.9881 is greater than the significant value of 0.05, so we fail to reject the null hypothesis and come to the conclusion that our data is not stationary. 

\textcolor{blue}{(b) If it is not stationary, determine the level of differencing to make our series stationary. We can use the ndiffs function which performs a unit-root test to determine this. After this, difference your data to ascertain a stationary time series. Re-do part a) for your differenced time series and comment on the time series plot, ACF and PACF. Recall that the time series models we’ve observed rely on stationarity.}

```{r}
ndiffs(cpi_ts)
```
```{r}
cpi_diff <- diff(cpi_ts, differences = 2)
```

The level of differencing to make our series stationary is 2. 

```{r}
tsdisplay(cpi_diff, main="CPI Time Series")
adf.test(cpi_diff)
```

The CPI time series fluctuates around a mean of 0 with no clear trend, which is a sign of stationarity. However, variance appears to increase slightly over time, especially after the 1980s, possibly reflecting heteroskedasticity. The shape of the ACF shows no significance pattern or sign of decay. The PACF shows several significant negative spikes up to around lag 10–12.


3. Feature Generation, Model Testing and Forecasting

\textcolor{blue}{(a) Fit an AR(p) model to the data (using part 2(a), AIC or some built in R function) and describe the model obtained.}

```{r}
library(dynlm)
SC <- 9999
results <- matrix("", nrow = 15, ncol = 1)
for (p in 1:15) {
  mdl <- dynlm(cpi_diff ~ L(cpi_diff, 1:p), data = cpi_diff)
  N <- nobs(mdl)
k <- 1 + p
 SC_new <- log(sum(mdl$residuals^2) / N) + k * log(N) / N
  if (SC_new < SC) {
    save_p <- p
    SC <- SC_new
  }
    results[p, 1] <- SC_new
}

results <- as.numeric(results)
selected_lag <- which.min(results)

print(results)

```
```{r}
AR_1 <- dynlm(cpi_diff ~ L(cpi_diff, 1:save_p), data = cpi_diff)
summary (AR_1)
```
We obtained an AR(11) model according to the SC values. The lowest SC value was on the 11th lag, which the value was -2.149196. Since our data is modeled monthly, this value can be interpreted to say that approximately the past 11 months of CPI value influences the current CPI.  

\textcolor{blue}{(b) Plot and comment on the ACF of the residuals of the model chosen in 3(a). If the model is properly fit, then we should see no autocorrelations in the residuals. Carry out a formal test for autocorrelation and comment on the results}.

```{r}
resid <- residuals(AR_1)
acf(resid, main = "ACF of Residuals from AR(11) Model")
library(lmtest)
bgtest(AR_1, order = 11)
```

There is only one significant spike at the first lag, so there is no significant autocorrelation as seen by the ACF of the residuals. However, the BG test tells us there is autocorrelation as seen by the small p-value of 0.001381 less than the significant value of 0.05. To fix this autocorrelation, we have to use the Newey West method to correct for the standard errors. 

```{r}
library(sandwich)
thefix <- coeftest(AR_1, vcov. = NeweyWest(AR_1))
summary(thefix)
```

\textcolor{blue}{(c) Using the appropriate predictors, fit an ARDL(p,q) model to the data and repeat step (b) in part 3.}

```{r}

results <- data.frame(p = integer(), q = integer(), BIC = numeric())

for (p in 1:20) {
  for (q in 0:5) {
    formula_str <- paste0(
      "cpi_ts ~ ",
      "L(cpi_ts, 1:", p, ")",
      if (q >= 0) paste0(" + L(time(cpi_ts), 0:", q, ")") else ""
    )

model <- dynlm(as.formula(formula_str))

results <- rbind(results, data.frame(p = p, q = q, BIC = BIC(model)))
  }
}

best_row <- results[which.min(results$BIC), ]
cat("Best ARDL(p,q): (", best_row$p, ",", best_row$q, ") with BIC =", 
    round(best_row$BIC, 2), "\n")
```


The results reveal an ARDL(13,0) model. Since we only have one variable other than time, we had use lagged values of CPI and the time to create an ARDL model. 

```{r}
ARDL <- dynlm(cpi_ts ~ L(cpi_ts, 1:13) + time(cpi_ts))
summary (ARDL)
```

```{r}
resid1 <- residuals(ARDL)
acf(resid, main = "Residuals from ARDL(13,0) Model")
library(lmtest)
bgtest(ARDL, order = 13)
```
There is only one significant spike at the first lag, so there is no significant autocorrelation as seen by the ACF of the residuals. However, the BG test tells us there is autocorrelation as seen by the small p-value of 1.348e-08 less than the significant value of 0.05. To fix this autocorrelation, we have to use the Newey West method to correct for the standard errors. 

```{r}
thefix1 <- coeftest(ARDL, vcov. = NeweyWest(ARDL))
summary(thefix1)
```


4. Provide a brief summary of your findings and state which model performs better.

We came to conclusion with two models: AR(11) and ARDL(13,0). While the ARDL(13,0) model provided the best in-sample fit as seen by high value of the adjusted R squared, the analysis of its residuals showed autocorrelation. This may have resulted from overfitting. The AR(11) model also showed autocorrelation as seen by the results in the BG test, but its residual behavior seems to be cleaner than the ARDL(13,0) model. Additionally, the SC/BIC level for the AR(11) model was much less than than for the ARDL(13,0) model, so the AR(11) model performs better. 

5. Suggest any limitations faced or improvements which could’ve been made to the model based on your findings, which should be supplemented with statistical tests(eg. degree of freedom restrictions, reverse causality).

The results of the ADF test indicate that the time series data for CPI are non-stationary. This suggests that the model may not adequately capture trends or seasonal changes in the data. As seen by the values of the Breusch-Godfrey test, there is autocorrelation in the residuals, which is a violation of the assumption of no autocorrelation. Lastly, there are fundamental limitations of this model because only CPI and time are used. Excluding macroeconomic components such as unemployment and money supply may bias coefficients. Seasonality may also have an impact because CPI changes seasonally.

We may use the moving average (MA) models to fix the error of autocorrelation. However, it is important to validate the necessity of the MA by confirming sharp declines in AIC and BIC indicators. Our AR(11) model did have a negative SV value, but it was a small negative value that may not be sufficient enough to consider using the MA model. 


## PART 2 - Time Series and Autocorrelation

Binary Dependent Variables

```{r}
diab_data <- read.csv("C:/Users/15654/Documents/diabetes_prediction_dataset.csv")

```

\textcolor{blue}{(a) Briefly discuss your data and the question you are trying to answer with your model.}

We got our dataset from Kaggle. The data used in this analysis is sourced from the Electronic Health Records (EHRs), and the specific dataset is owned and managed by Mohammed Mustafa. 
Mustafa, Mohammed. (2023). *Diabetes prediction dataset* [Data set]. https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset?resource=download

The question we are trying to answer is, "Can we predict whether a person has diabetes based on their health and demographic features?" This data set is a binary classification that estimates the likelihood that a person is diabetic based on features such as age, BMI, and health measures. The explanatory variables include age, gender, hypertension, heart disease, smoking history, and BMI. Age is a continuous variable, ranging from 0 to 80 in this data set. Gender is a categorical variable; there is male, female, and other. Hypertension is a binary variable where 0 indicates the individual doesn’t have hypertension, and 1 indicates that the individual has hypertension. Heart disease is also a binary variable where 0 indicates the individual doesn’t have heart disease, and 1 indicates that the individual has a heart disease. Smoking history is a categorical variable with categories of not current, former, no info, current, never, and ever. Body mass index (BMI) is a continuous variable. BMI less than 18.5 is underweight, 18.5-24.9 is normal, 25-29.9 is overweight, and 30 or more is obese.

The binary dependent variable is whether a person has diabetes or not, with values of 1 indicating the presence of diabetes and 0 indicating the absence of diabetes. 

\textcolor{blue}{(b) Provide a descriptive analysis of your variables. This should include RELEVANT histograms and fitted distributions, correlation plot, boxplots, scatterplots, and statistical summaries (e.g., the five-number summary). All figures must include comments. For binary variables, you can simply include the proportions of each factor.}

\textcolor{blue}{Age}
```{r age, fig.width=5, fig.height=4}
library(ggplot2)
ggplot(diab_data, aes(x = age)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "skyblue", 
                 color = "black") +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Histogram of Age with Density Curve", x = "Age", y = "Density")

boxplot(diab_data$age, main = "Boxplot of Age", col = "lightblue")

summary(diab_data$age)
```
The age variable is an important factor as diabetes is more commonly diagnosed in older adults. The histogram appears to be slightly bimodal, and skewed slightly to the right. As seen from the boxplot and the five number summary, age ranges from 0-80 in this data set, and the IQR is 36. There are no outliers. There is a mean of 41.89 and a median of 43. 

\textcolor{blue}{Gender}
```{r gender, fig.width=5, fig.height=4}

ggplot(diab_data, aes(x = gender)) +
  geom_bar(fill = "lightblue") +
  ggtitle("Gender Distribution")

prop.table(table(diab_data$gender)) * 100
```
Gender can have an impact on their susceptibility to diabetes. There are three categories: male, female, and other. 58.55\% of the individuals are female, 41.43\% are male, and 1.8% selected "other" as their classification of gender. 

\textcolor{blue}{Hypertension}
```{r hypertension, fig.width=5, fig.height=4}

ggplot(diab_data, aes(x = factor(hypertension))) +
  geom_bar(fill = "lightblue") +
  xlab("Hypertension (0 = No, 1 = Yes)") +
  ylab("Count") +
  ggtitle("Hypertension")

prop.table(table(diab_data$hypertension)) * 100
```
Hypertension is a medical condition in which the blood pressure in the arteries is persistently elevated. It has values of 0 or 1 where 0 indicates the individual doesn’t have hypertension, and 1 indicates that they have hypertension. 92.515\% of the individuals responded that they do not have hypertension, and 7.485\% of the individuals responded that they do have hypertension. 

\textcolor{blue}{Heart Disease}
```{r heart disease, fig.width=5, fig.height=4}

ggplot(diab_data, aes(x = factor(heart_disease))) +
  geom_bar(fill = "lightblue") +
  xlab("Heart Disease (0 = No, 1 = Yes)") +
  ylab("Count") +
  ggtitle("Heart Disease")

prop.table(table(diab_data$heart_disease)) * 100
```

Heart disease is another medical condition that is associated with an increased risk of developing diabetes. It is a binary variable where 0 indicates the individual doesn’t have heart disease, and 1 indicates that the individual has a heart disease. 96.058\% of the individuals responded that they do not have a heart disease, and 3.942\% of the individuals responded that they do have heart disease. 

\textcolor{blue}{Smoking History|
```{r smoking, fig.width=5, fig.height=4}

ggplot(diab_data, aes(x = smoking_history)) +
  geom_bar(fill = "lightblue") +
  ggtitle("Smoking History")

prop.table(table(diab_data$smoking_history)) * 100
```
Smoking history is considered a risk factor for diabetes. There are 6 categories: not current, former, never, and ever, no info, and not current. The category "Current" indicates that the individual is actively smoking or has reported smoking recently. The category "not current" refers to individuals who used to smoke but are currently not smoking. The category "former" refers to individuals who used to smoke but are currently not smoking and have been abstinent for a longer period of time than those in the "not current" category. The category "never" refers to individuals who have never smoked in their life. The category "Ever" represents individuals who have ever smoked in their lifetime, regardless of their current smoking status. 9.286\% of the individuals responded that they are current smokers, while 9.352\% are former smokers. 35.095\% have never smoked, and 35.816\% gave no information regarding their smoking history. 6.447\% responded that they are no longer smokers, and 4.004\% have smoked at least once in their life. 

\textcolor{blue}{Body Mass Index (BMI)}
```{r BMI, fig.width=5, fig.height=4}
library(ggplot2)
ggplot(diab_data, aes(x = bmi)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "skyblue", 
                 color = "black") +
  geom_density(color = "red", linewidth = 1) +
  labs(title = "Histogram of BMI with Density Curve", x = "BMI", y = "Density")

boxplot(diab_data$bmi, main = "Boxplot of BMI", col = "lightblue", 
        horizontal = TRUE)

summary(diab_data$bmi)
```
BMI (Body Mass Index) is a measure of body fat based on weight and height. Higher BMI values are linked to a higher risk of diabetes. BMI less than 18.5 is underweight, 18.5-24.9 is normal, 25-29.9 is overweight, and 30 or more is obese. The range of BMI in the dataset is from 10.01 to 95.69, and the IQR is 5.95. The histogram is approximately bell-shaped with a slight right skew. Most values are centered around the median of 27.32, but the tail on the right side extends further, indicating the presence of individuals with unusually high BMI values. The boxplot reveals that there are many outliers, both on the lower and higher sides. The mean is also 27.32, equal to the median. 

\textcolor{blue}{(c) Fit the three models (Linear Probability Model, Logit Model, and Probit Model), and identify which model is your preferred one and why. Make sure to include statistical diagnostics to support your conclusion, and to comment on your findings.}

```{r}
lpm_model <- lm(diabetes ~ age + bmi + gender + hypertension + heart_disease
                + smoking_history, 
                data = diab_data)
summary(lpm_model)
hcErrors <- coeftest(lpm_model, vcov.=hccm(lpm_model, type = "hc1"))
hcErrors
```


```{r}
logit_model <- glm(diabetes ~ age + bmi + gender + hypertension + heart_disease
                   + smoking_history, 
                   data = diab_data, family = binomial(link = "logit"))
summary(logit_model)

probit_model <- glm(diabetes ~ age + bmi + gender + hypertension + heart_disease
                    + smoking_history, 
                    data = diab_data, family = binomial(link = "probit"))
summary(probit_model)
```

```{r}
comparison <- data.frame(
  Model = c("LPM", "Logit", "Probit"),
  AIC = c(AIC(lpm_model), AIC(logit_model), AIC(probit_model)),
  BIC = c(BIC(lpm_model), BIC(logit_model), BIC(probit_model)),
  Pseudo_R2 = c(NA, pR2(logit_model)["McFadden"], pR2(probit_model)["McFadden"])
)

comparison

```

The AIC and BIC levels show that the LPM is the best model out of the three choices because AIC and BIC values for LPM are the lowest. 
